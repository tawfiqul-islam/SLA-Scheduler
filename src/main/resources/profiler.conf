#Number of Worker Nodes
worker.numbers=3

###Configuration of each Worker Machine

#total usable cpu cores in each worker ** can be taken from spark-env.sh file in future
worker.cores=15

#total usable memory in each worker (in GB) ** can be taken from spark-env.sh file in future
worker.memory=52

#Maximum limit of cpu cores per executor (it will be 5 for now as recommended by Spark developers)
executor.cores.limit=5

coreCost=0.1025
#Input Size for Profiling (in GB)

profiler.input.size=1

#times to do reprofiling
reprofile.size=3

#Algorithm to be used with profiler
profiler.algorithm=WordCount

#Profiling Level (1. high = use all possible combination of cores for executors(1~5), it will take a lot of time to profile;
# 				  2. low = only use default cores limit(5) per executor)

profiler.level=low
#Path for Spark Home Directory
spark.home=/home/tawfiq/sp/spark-2.0.1

#Path for Profiler Input File
profiler.input.path=/home/tawfiq/sp/spark-2.0.1/myinput

#Path for Application Input File
application.input.path=/home/tawfiq/sp/spark-2.0.1/myappinput

#Path for Application Jar File
application.jar.path=/home/tawfiq/sp/spark-2.0.1/bigdatabench-spark_1.3.0-hadoop_1.0.4.jar

#application arguments
appArgs=prize

#Class to use of Application
application.class=cn.ac.ict.bigdatabench.WordCount

#path to store output of Application
application.outputPath=/home/tawfiq/sp/spark-2.0.1/myoutput


#spark master ip

spark.master=spark://20.0.0.4:7077


